load classes sp.jar;

file -inlinebatch END_OF_BATCH

-- Partitioned Data Table
%%TABLE name=partitioned_table, primary_key=rowid, partition_key=rowid

-- Index over rowid_group on Partitioned Data Table
CREATE INDEX IX_partitioned_table_rowid_group
    ON partitioned_table ( rowid_group );

-- Grouping view over Partitioned Data Table
CREATE VIEW partitioned_table_group
(
  rowid_group
, record_count
)
AS
   SELECT rowid_group
        , COUNT(*)
     FROM partitioned_table
 GROUP BY rowid_group;

-- Export Table for Partitioned Data Table deletions
%%TABLE name=export_partitioned_table_kafka, partition_key=rowid, migrate_target=kafka_target, ttl_index=type_not_null_timestamp, ttl_value="5 SECONDS"
CREATE INDEX export_partitioned_table_kafka_idx ON  export_partitioned_table_kafka(type_not_null_timestamp) where not migrating;

%%TABLE name=export_partitioned_table_file, partition_key=rowid, migrate_target=file_target, ttl_index=type_not_null_timestamp, ttl_value="5 SECONDS"
CREATE INDEX export_partitioned_table_file_idx ON  export_partitioned_table_file(type_not_null_timestamp) where not migrating;

%%TABLE name=export_partitioned_table_jdbc, partition_key=rowid, migrate_target=jdbc_target, ttl_index=type_not_null_timestamp, ttl_value="5 SECONDS"
CREATE INDEX export_partitioned_table_jdbc_idx ON  export_partitioned_table_jdbc(type_not_null_timestamp)  where not migrating;

%%TABLE name=export_mirror_partitioned_table, partition_key=rowid, ttl_index=type_not_null_timestamp, ttl_value="5 SECONDS"

%%TABLE name=export_mirror_replicated_table, ttl_index=type_not_null_timestamp, ttl_value="5 SECONDS"

CREATE TABLE migrate_done_table_topic EXPORT TO TARGET kafka_topic_done
(
  txnid                     BIGINT        NOT NULL 
);

CREATE STREAM export_done_table_kafka EXPORT TO TARGET kafka_target
(
  txnid                     BIGINT        NOT NULL
);

CREATE STREAM export_done_table_jdbc EXPORT TO TARGET jdbc_target
(
  txnid                     BIGINT        NOT NULL
);

CREATE STREAM export_done_table_file EXPORT TO TARGET file_target
(
  txnid                     BIGINT        NOT NULL
);

-- Export Table for Replicated Data Table deletions
%%TABLE name=export_replicated_table_kafka, migrate_target=kafka_target, ttl_index=type_not_null_timestamp, ttl_value="5 SECONDS"
CREATE INDEX export_replicated_table_kafka_idx ON export_replicated_table_kafka(type_not_null_timestamp) where not migrating;

%%TABLE name=export_replicated_table_file, migrate_target=file_target, ttl_index=type_not_null_timestamp, ttl_value="5 SECONDS"
CREATE INDEX export_replicated_table_file_idx ON export_replicated_table_file(type_not_null_timestamp) where not migrating;

%%TABLE name=export_replicated_table_jdbc, migrate_target=jdbc_target, ttl_index=type_not_null_timestamp, ttl_value="5 SECONDS"
CREATE INDEX export_replicated_table_jdbc_idx ON export_replicated_table_jdbc(type_not_null_timestamp) where not migrating;

CREATE PROCEDURE PARTITION ON TABLE export_partitioned_table_kafka COLUMN rowid PARAMETER 0 FROM CLASS genqa.procedures.JiggleExportGroupSinglePartition;
CREATE PROCEDURE PARTITION ON TABLE partitioned_table COLUMN rowid PARAMETER 0 FROM CLASS genqa.procedures.JiggleSinglePartition;
CREATE PROCEDURE PARTITION ON TABLE export_partitioned_table_kafka COLUMN rowid PARAMETER 0 FROM CLASS genqa.procedures.JiggleExportSinglePartition;
CREATE PROCEDURE FROM CLASS genqa.procedures.InsertExportDoneDetails;
CREATE PROCEDURE FROM CLASS genqa.procedures.InsertMigrateDoneDetails;

CREATE PROCEDURE FROM CLASS genqa.procedures.JiggleExportMultiPartition;

CREATE PROCEDURE SelectwithLimit as select * from export_mirror_partitioned_table where rowid between ? and ? order by rowid limit ?;

-- Export Stream with extra Geo columns
%%STREAM name=export_geo_partitioned_table_jdbc, partition_key=rowid, export_target=jdbc_target, geocolumns=True

-- should be an exact copy of the stream. Used for verifiing
-- export stream contents.
%%TABLE name=export_geo_mirror_partitioned_table, partition_key=rowid, geocolumns=True

CREATE STREAM export_geo_done_table_kafka EXPORT TO TARGET kafka_target
(
  txnid                     BIGINT        NOT NULL
);

CREATE STREAM export_geo_done_table_jdbc EXPORT TO TARGET jdbc_target
(
  txnid                     BIGINT        NOT NULL
);

CREATE VIEW EXPORT_PARTITIONED_TABLE_VIEW_KAFKA
(
  rowid
, record_count
)
AS
   SELECT rowid
        , COUNT(*)
     FROM EXPORT_PARTITIONED_TABLE_KAFKA
 GROUP BY rowid;

CREATE VIEW EXPORT_PARTITIONED_TABLE_VIEW_JDBC
(
  rowid
, record_count
)
AS
   SELECT rowid
        , COUNT(*)
     FROM EXPORT_PARTITIONED_TABLE_JDBC
 GROUP BY rowid;

-- this is analogous to JiggleExportSinglePartition to insert tuples, but has the extra 4 geo columns
CREATE PROCEDURE PARTITION ON TABLE export_geo_partitioned_table_jdbc COLUMN rowid PARAMETER 0 FROM CLASS genqa.procedures.JiggleExportGeoSinglePartition;

-- this is used by the verifier inside JDBCGetData, re-point to the geo tables
-- DROP PROCEDURE SelectwithLimit IF EXISTS;
-- CREATE PROCEDURE SelectwithLimit as select * from export_geo_mirror_partitioned_table where rowid between ? and ? order by rowid limit ?;


END_OF_BATCH
