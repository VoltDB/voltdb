load classes sp.jar;

file -inlinebatch END_OF_BATCH

-- Partitioned Data Table
%%TABLE name=partitioned_table, partition_key=rowid, primary_key=rowid

-- Index over rowid_group on Partitioned Data Table
CREATE INDEX IX_partitioned_table_rowid_group
    ON partitioned_table ( rowid_group );

-- Grouping view over Partitioned Data Table
CREATE VIEW partitioned_table_group
(
  rowid_group
, record_count
)
AS
   SELECT rowid_group
        , COUNT(*)
     FROM partitioned_table
 GROUP BY rowid_group;

-- Change data capture (CDC) table used with "tableexport" test cases
%%TABLE name=export_partitioned_table_cdc, export_target=kafka_target, export_cdc="insert, update, delete", partition_key=rowid

-- Export Table for Partitioned Data Table deletions
%%STREAM name=export_partitioned_table_kafka, export_target=kafka_target, partition_key=rowid

%%STREAM name=export_partitioned_table_rabbit, export_target=rabbit_target, partition_key=rowid

%%STREAM name=export_partitioned_table_file, export_target=file_target, partition_key=rowid

%%STREAM name=export_partitioned_table_jdbc, export_target=jdbc_target, partition_key=rowid

%%STREAM name=topic_stream, topic=True, partition_key=rowid

%%TABLE name=export_mirror_partitioned_table, partition_key=rowid

%%TABLE name=export_mirror_replicated_table

-- TODO single export_done table with: VARCHAR(128) tablename, BIGINT txnid, BIGINT rowid

CREATE STREAM export_done_table_kafka EXPORT TO TARGET kafka_target
(
  txnid                     BIGINT        NOT NULL
);

CREATE STREAM export_done_table_rabbit EXPORT TO TARGET rabbit_target
(
  txnid                     BIGINT        NOT NULL
);

CREATE STREAM export_done_table_jdbc EXPORT TO TARGET jdbc_target
(
  txnid                     BIGINT        NOT NULL
);

CREATE STREAM export_done_table_file EXPORT TO TARGET file_target
(
  txnid                     BIGINT        NOT NULL
);

-- Export Table for Replicated Data Table deletions
%%STREAM name=export_replicated_table_kafka, export_target=kafka_target

%%STREAM name=export_replicated_table_rabbit, export_target=rabbit_target

%%STREAM name=export_replicated_table_file, export_target=file_target

%%STREAM name=export_replicated_table_jdbc, export_target=jdbc_target

%%TABLE name=export_replicated_table_cdc, export_target=kafka_target, export_cdc="insert, update, delete"

CREATE PROCEDURE PARTITION ON TABLE export_partitioned_table_kafka COLUMN rowid PARAMETER 0 FROM CLASS genqa.procedures.JiggleExportGroupSinglePartition;
CREATE PROCEDURE PARTITION ON TABLE topic_stream COLUMN rowid PARAMETER 0 FROM CLASS genqa.procedures.InsertTopicStream;
CREATE PROCEDURE PARTITION ON TABLE partitioned_table COLUMN rowid PARAMETER 0 FROM CLASS genqa.procedures.JiggleSinglePartition;
CREATE PROCEDURE PARTITION ON TABLE export_partitioned_table_kafka COLUMN rowid PARAMETER 0 FROM CLASS genqa.procedures.JiggleExportSinglePartition;
CREATE PROCEDURE FROM CLASS genqa.procedures.InsertExportDoneDetails;

CREATE PROCEDURE FROM CLASS genqa.procedures.JiggleExportMultiPartition;

CREATE PROCEDURE PARTITION ON TABLE export_partitioned_table_cdc COLUMN rowid PARAMETER 0 FROM CLASS genqa.procedures.TableExport;

CREATE PROCEDURE SelectwithLimit as select * from export_mirror_partitioned_table where rowid between ? and ? order by rowid limit ?;
CREATE PROCEDURE SelectGeowithLimit as select * from export_geo_mirror_partitioned_table where rowid between ? and ? order by rowid limit ?;

-- Export Stream with extra Geo columns
%%STREAM name=export_geo_partitioned_table_jdbc, partition_key=rowid, export_target=jdbc_target, geocolumns=True

-- should be an exact copy of the stream. Used for verifiing
-- export stream contents.
%%TABLE name=export_geo_mirror_partitioned_table, partition_key=rowid, geocolumns=True

CREATE STREAM export_geo_done_table_kafka EXPORT TO TARGET kafka_target
(
  txnid                     BIGINT        NOT NULL
);

CREATE STREAM export_geo_done_table_jdbc EXPORT TO TARGET jdbc_target
(
  txnid                     BIGINT        NOT NULL
);

CREATE VIEW EXPORT_PARTITIONED_TABLE_VIEW_KAFKA
(
  rowid
, record_count
)
AS
   SELECT rowid
        , COUNT(*)
     FROM EXPORT_PARTITIONED_TABLE_KAFKA
 GROUP BY rowid;

CREATE VIEW EXPORT_PARTITIONED_TABLE_VIEW_JDBC
(
  rowid
, record_count
)
AS
   SELECT rowid
        , COUNT(*)
     FROM EXPORT_PARTITIONED_TABLE_JDBC
 GROUP BY rowid;

-- this is analogous to JiggleExportSinglePartition to insert tuples, but has the extra 4 geo columns
CREATE PROCEDURE PARTITION ON TABLE export_geo_partitioned_table_jdbc COLUMN rowid PARAMETER 0 FROM CLASS genqa.procedures.JiggleExportGeoSinglePartition;

-- this is used by the verifier inside JDBCGetData, re-point to the geo tables
-- DROP PROCEDURE SelectwithLimit IF EXISTS;
-- CREATE PROCEDURE SelectwithLimit as select * from export_geo_mirror_partitioned_table where rowid between ? and ? order by rowid limit ?;

END_OF_BATCH
